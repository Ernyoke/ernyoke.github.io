<html data-theme="light" lang="en">
<head>
<title>Running Serverless Lambdas with Rust on AWS - ervinszilagyi.dev</title>
<link href="https://ervinszilagyi.dev/images/logo48.png" rel="icon" type="image/png">
<link href="https://ervinszilagyi.dev/styles/styles.css" rel="stylesheet">
<link href="https://ervinszilagyi.dev/styles/all.min.css" rel="stylesheet">
<link href="https://fonts.gstatic.com" rel="preconnect">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i,700,700i,900,900i&amp;display=swap" rel="stylesheet">
<link href="https://ervinszilagyi.dev/styles/atom-one.css" rel="stylesheet" title="light">
<link disabled href="https://ervinszilagyi.dev/styles/atom-one-dark.css" rel="alternate stylesheet" title="dark">
<script src="https://ervinszilagyi.dev/scripts/scripts.js"></script>
<script data-domain="ervinszilagyi.dev" defer src="https://plausible.io/js/plausible.js"></script>
<meta charset="utf-8">
<meta content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0" name="viewport">
<meta content="ie=edge" http-equiv="X-UA-Compatible">
<script>
        const isDarkMode = () => window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        let theme = localStorage.getItem("theme");
        if (!theme) {
            theme = isDarkMode() ? "dark" : "light";
        }
        document.documentElement.setAttribute("data-theme", theme);
    </script>
<meta content="Running Serverless Lambdas with Rust on AWS" property="og:title">
<meta content="Personal web page and blog." property="og:description">
<meta content="https://ervinszilagyi.dev/running-serverless-lambdas-with-rust-aws.html" property="og:url">
<meta content="https://ervinszilagyi.dev/images/cover.png" property="og:image">
<meta content="Running Serverless Lambdas with Rust on AWS" property="twitter:title">
<meta content="Personal web page and blog." property="twitter:description">
<meta content="@ervin_szilagyi" property="twitter:site">
<meta content="@ervin_szilagyi" property="twitter:creator">
<meta content="https://ervinszilagyi.dev/images/cover.png" property="twitter:image">
<meta content="summary_large_image" property="twitter:card">
</head>
<body class="light-mode">
<nav class="menu">
<div class="nav-left">
<ul>
<li>
<a href="https://ervinszilagyi.dev/">
<img alt="logo" class="logo" src="https://ervinszilagyi.dev/images/logo48.png">
<span class="sitename">Ervin Szilágyi</span>
</a>
</li>
</ul>
</div>
<div class="nav-right">
<ul>
<li>
<a class="fa fa-circle" id="themeSwitcher" onclick="switchTheme()"></a>
</li>
<li>
<a aria-hidden="true" href="https://ervinszilagyi.dev/resume.html">Résumé</a>
</li>
<li>
<a aria-hidden="true" href="https://ervinszilagyi.dev/projects.html">Projects</a>
</li>
<li>
<a aria-hidden="true" href="https://ervinszilagyi.dev/blog.html">Blog</a>
</li>
</ul>
</div>
</nav>
<article id="main-content"><h1>Running Serverless Lambdas with Rust on AWS</h1>
<h2>Intro<a class="anchor-link" href="#intro" id="intro">&lt;&lt;</a>
</h2>
<p>Recently I started playing around with Rust programming language. Currently, there is a Rust bandwagon on which I decided to jump, but ultimately I don't regret it. I consider Rust to be pretty unique compared with the languages (Java, JavaScript, Python) I work on a daily basis. Since at my current job I have to build and maintain a bunch of Lambda functions, I was thinking about bringing it to the cloud to see how it fairs compared to what I'm used to.</p>
<h2>Lambda Runtime Environment for Rust<a class="anchor-link" href="#lambda-runtime-environment-for-rust" id="lambda-runtime-environment-for-rust">&lt;&lt;</a>
</h2>
<p>If we take a look at the <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html" target="_blank">AWS documentation</a> for Lambda runtimes, we can see that Rust is not on the list of supported languages out of the box. Well, that's maybe discouraging at first, but it turns out this is not a huge issue at all. There absolutely is support for Rust, in fact, the <a href="https://aws.amazon.com/blogs/opensource/rust-runtime-for-aws-lambda/" target="_blank">official blog post</a> announcing the Rust implementation for the Lambda runtime is from more than 4 years ago. Using this runtime implementation we can build an executable for a Custom Lambda Environment.</p>
<p>The Lambda runtime implementation for Rust is a dependency (which in the world of Rust are called crates), that will be packaged with our code.</p>
<h2>Create a Lambda Project using Rust<a class="anchor-link" href="#create-a-lambda-project-using-rust" id="create-a-lambda-project-using-rust">&lt;&lt;</a>
</h2>
<p>The easiest way to create a Lambda project for Rust language is to use <a href="https://www.cargo-lambda.info/guide/what-is-cargo-lambda.html" target="_blank"><code>cargo-lambda</code></a>. This can be installed on any OS following the instruction from the <a href="https://www.cargo-lambda.info/guide/installation.html" target="_blank">documentation</a>. To create a new project, we can use this command:</p>
<pre><code class="language-bash">cargo lambda new function-name
</code></pre>
<p>Running the command it will prompt some questions for us asking about the event type we will plan to use to launch the Lambda. Afterward, it will generate the project including the required AWS SDK dependencies based on what trigger we chose. For example, for a function that can be invoked by an API Gateway - REST endpoint, <code>cargo lambda</code> will generate something like this:</p>
<pre><code class="language-Toml">[package]
name = "fn-test"
version = "0.1.0"
edition = "2021"

[dependencies]
lambda_http = { version = "0.6.0", default-features = false, features = ["apigw_rest"] }
lambda_runtime = "0.6.0"
tokio = { version = "1", features = ["macros"] }
tracing = { version = "0.1", features = ["log"] }
tracing-subscriber = { version = "0.3", default-features = false, features = ["fmt"] }
</code></pre>
<p>We can notice that the generated code includes <code>tokio</code> crate. The Lambda runtime and the AWS API for Rust rely heavily on <code>tokio</code> for asynchronous calls and invocations. We get the following handler for our Lambda as well:</p>
<pre><code class="language-Rust">use lambda_http::{run, service_fn, Body, Error, Request, RequestExt, Response};

async fn function_handler(event: Request) -&gt; Result&lt;Response&lt;Body&gt;, Error&gt; {
    let resp = Response::builder()
        .status(200)
        .header("content-type", "text/html")
        .body("Hello AWS Lambda HTTP request".into())
        .map_err(Box::new)?;
    Ok(resp)
}

#[tokio::main]
async fn main() -&gt; Result&lt;(), Error&gt; {
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(false)
        .without_time()
        .init();

    run(service_fn(function_handler)).await
}
</code></pre>
<h2>Build a Deployable Package<a class="anchor-link" href="#build-a-deployable-package" id="build-a-deployable-package">&lt;&lt;</a>
</h2>
<p>In order to build our Lambda, we can also rely on the help of <code>cargo-lambda</code> tool. The following command will build the project for fetching all the crates:</p>
<pre><code>cargo lambda build --release
</code></pre>
<p>By default, this will target the <code>x86-64</code> architecture. If we would like to target Graviton with ARM64 architecture, we can add the <code>--arm64</code> flag. It is that simple. We can notice that, that the architecture of our current machine does not matter when building the Lambda. We can create a target package for <code>x86</code> or for <code>arm64</code>, regardless if we are on a shiny MacBook M1 machine or an intel/AMD based PC/laptop. <code>cargo-lambda</code> uses Zig-lang under the hood for cross-compilation and it makes sure to hide the complexity from us when we are building Rust executables.</p>
<h3>What if we don't like magic?</h3>
<p>Using <code>cargo-lambda</code> makes it easy to build the necessary executable from a Rust project, but what if we would want to do it ourselves? It turns out this can be a kind of rabbit hole. Generally, if our Lambda is a binary executable, it should be built targeting a Linux environment since AWS is using a flavor of Amazon Linux under the hood for the Lambda runtime. This can be done somewhat flawlessly if our host machine is also running Linux. We just have to do the steps below:</p>
<ol>
<li>Assuming we have Rust installed the <a href="https://www.rust-lang.org/tools/install" target="_blank">recommended way</a>, we should also have <code>rustup</code> tool present for managing build toolchains. Using <code>rustup</code> we should add <code>x86_64-unknown-linux-musl</code> build target:</li>
</ol>
<pre><code>rustup target add x86_64-unknown-linux-musl
</code></pre>
<ol start="2">
<li>Now we can use <code>cargo</code> to build our project in release mode:</li>
</ol>
<pre><code>cargo build --release --target x86_64-unknown-linux-musl
</code></pre>
<p>This will create a <code>target</code> folder in which we will have a binary ready to be deployed to AWS.</p>
<h3>What about Windows and macOS?</h3>
<p>If we are running Windows, we can use WSL to have access to a fully-fledged Linux environment, and then we can do the steps above. In case we don't want to use WSL or for some reason, we can't use it, we will have to do a cross-build to Linux. I tried the steps above from Windows, but at each attempt, I was running into a linker issue (<code>error: linker cc not found</code>). I am sure that are several ways to solve this, what I found to be the easier solution is to use <a href="https://github.com/cross-rs/cross" target="_blank"><code>cross</code></a>. Before somebody screams at me... I know this solution is kind of a band-aid since <code>cross</code> will rely on Docker to target different environments...but hey, it worked for my purpose.</p>
<p>To install <code>cross</code>, we can do the following:</p>
<pre><code class="language-bash">cargo install cross --git https://github.com/cross-rs/cross
</code></pre>
<p>Then we can use <code>cross</code> to build the project:</p>
<pre><code class="language-bash">cross build --release --target x86_64-unknown-linux-musl
</code></pre>
<p>In fact, if we want our Lambda to run on Graviton, we can very easily target ARM with <code>--target aarch64-unknown-linux-gnu</code> flag. Moreover, <code>cross</code> can be used from MacOS (M1 and intel) to target both <code>x84</code> and <code>arm64</code> architectures.</p>
<h2>Deploy the Package to AWS<a class="anchor-link" href="#deploy-the-package-to-aws" id="deploy-the-package-to-aws">&lt;&lt;</a>
</h2>
<p>The super-easy way would be to rely on <code>cargo-lambda</code> again. What we have to do is to execute the command below:</p>
<pre><code class="language-bash">cargo lambda deploy
</code></pre>
<p>That's it. Obviously, this assumes we have an AWS CLI setup locally (we can do this by installing <code>aws-cli</code> and running <code>aws configure</code>). This will provision our Lambda in AWS and will also create an execution IAM role for it. We can override this behavior and use a pre-existing IAM role if we want by specifying the role ARN for the <code>--iam-role</code> flag.</p>
<p>If we want to live a bit more "dangerously", and do the deployment ourselves, we essentially have to do a few more steps. Assuming that we have successfully done the manual build steps from above, first we have to compress the executable into a zip archive:</p>
<pre><code class="language-bash"># Run it from the a root folder of the project
cp target/x86_64-unknown-linux-musl/release/your_lambda_app_name ./bootstrap
zip lambda.zip bootstrap
</code></pre>
<p>Using the AWS CLI, we can create a Lambda and deploy the archive in one step:</p>
<pre><code class="language-bash">aws lambda create-function --function-name your_lambda_app_name \
    --runtime provided.al2 --zip-file fileb://lambda.zip \
    --handler bootstrap --role arn:aws:iam::123456789668:role/service-role \
    --architectures x86_64
</code></pre>
<p>Additional notes regarding <code>create-function</code>:</p>
<ul>
<li>Since we use a custom runtime, we can choose between <code>provided</code> and <code>provided.al2</code>. It is recommended to use <code>provided.al2</code> because this is the newer version;</li>
<li>The service role has to be created in advance. <code>cargo-lambda</code> can do this automatically as we've seen this before. Here we have to do it ourselves;</li>
<li>For the <code>architectures</code> we can choose either <code>x86_64</code> or <code>arm64</code>. We are providing an executable built for a specific architecture, we have to choose accordingly. If we don't specify anything, <code>x86_64</code> is used by default.</li>
</ul>
<p>If this command runs successfully, we can invoke our Lambda. Moreover, we should configure the necessary services for the trigger, but these steps fall outside of the topic of this article.</p>
<h2>Working with Containers<a class="anchor-link" href="#working-with-containers" id="working-with-containers">&lt;&lt;</a>
</h2>
<p>A while ago AWS <a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support" target="_blank">announced container support</a> for Lambda. This means a Lambda project can be packaged into a Docker container and pushed to a container registry (ECR), from where it can be deployed to AWS Lambda. Containerizing a Lambda function comes with a few benefits, for example, we can deploy a project with a size of up to 10GB. This can be helpful for a Rust project because having a bunch of dependencies can increase the project size exponentially.</p>
<p>AWS provides a base image for Lambda, <a href="https://gallery.ecr.aws/lambda/provided" target="_blank"><code>public.ecr.aws/lambda/provided:al2</code></a> which contains all the required components to run our functions. To containerize our Lambda, what we have to do is to copy our executable to the image and name it <code>bootstrap</code> (this name can be customized, but for sake of simplicity we won't do that). <code>provided:al2</code> has an entrypoint already configured to run the executable.</p>
<p>Taking our image a step further, we can build a <a href="https://docs.docker.com/develop/develop-images/multistage-build" target="_blank">multi-stage</a> Docker image. To accomplish this, we can use a base Rust image as the build stage, the purpose of which is to do the compile and build step and to produce an executable. This executable will be copied over to the main image by the next second stage. While a multi-stage Docker image is not mandatory for our purposes, it can be helpful to automate the whole process of building Rust Lambdas.</p>
<p>Without further ado, this is our Lambda container:</p>
<pre><code class="language-dockerfile">FROM rust:1.63-buster as builder

WORKDIR /build

ADD Cargo.toml Cargo.toml
ADD Cargo.lock Cargo.lock
ADD src src

RUN rustup target add x86_64-unknown-linux-musl &amp;&amp; cargo build --release --target x86_64-unknown-linux-musl

# copy artifacts to a clean image
FROM public.ecr.aws/lambda/provided:al2

COPY --from=builder /build/target/x86_64-unknown-linux-musl/release/bootstrap /bootstrap

ENTRYPOINT ["/bootstrap"]
</code></pre>
<p>This will create an <code>x86</code> Docker container, if we want an <code>arm64</code> based one, we can do it with a few modifications:</p>
<pre><code class="language-dockerfile">FROM rust:1.63-buster as builder

WORKDIR /build

ADD Cargo.toml Cargo.toml
ADD Cargo.lock Cargo.lock
ADD src src

RUN rustup target add aarch64-unknown-linux-musl &amp;&amp; cargo build --release --target aarch64-unknown-linux-musl

# copy artifacts to a clean image
FROM public.ecr.aws/lambda/provided:al2-arm64

COPY --from=builder /build/target/aarch64-unknown-linux-musl/release/bootstrap /bootstrap

ENTRYPOINT ["/bootstrap"]
</code></pre>
<p>For being able to define the architecture for a Docker image when building it, we would want to use <code>buildx</code> tool. This can be as easy as this:</p>
<pre><code class="language-bash">docker buildx build --progress=plain --platform linux/arm64 -t my-container-name .
</code></pre>
<p>The platform can be either <code>linux/amd64</code> or <code>linux/arm64</code>. By default, Docker will use the host machine's architecture, if we want to build an <code>x86-64</code> Lambda container for a Mac M1, we are required to specify the platform flag.</p>
<h2>Benchmarks<a class="anchor-link" href="#benchmarks" id="benchmarks">&lt;&lt;</a>
</h2>
<p>Rust is fast. It should be, it is a system's programming language compiled down to machine code for a specific platform. To see how fast this highly optimized machine code is when executed as an AWS Lambda, I decided to do some totally non-scientific benchmarks. </p>
<p>To measure the performance of code written in Rust I decided to attempt to compute the first N digits of PI. Algorithms used to compute digits of PI are used by several CPU stress testing and benchmarking tools, such as <a href="https://www.superpi.net/" target="_blank">https://www.superpi.net/</a>. </p>
<p>In order to compute the first N digits of PI, I used the <a href="http://www.cs.ox.ac.uk/jeremy.gibbons/publications/spigot.pdf" target="_blank">Unbounded Spigot Algorithms for the Digits of PI</a>. To be honest, I stole the JavaScript implementation from <a href="https://stackoverflow.com/a/64286624/7661119" target="_blank">this</a> StackOverflow answer and I re-wrote it in Rust. I don't want to claim that this is the faster algorithm to compute PI and certainly I don't want to claim that my Rust implementation of this algorithm is the most optimal. But I feel like my attempt was good enough to compare the performance of Rust to other languages such as JavaScript and draw some conclusions.</p>
<p>Here is my Rust implementation of the algorithm:</p>
<pre><code class="language-rust">fn generate_pi(limit: i32) -&gt; Vec&lt;i32&gt; {
    let mut q = 1.to_bigint().unwrap();
    let mut r = 180.to_bigint().unwrap();
    let mut t = 60.to_bigint().unwrap();
    let mut i = 2.to_bigint().unwrap();
    let mut res: Vec&lt;i32&gt; = Vec::new();
    for _ in 0..limit {
        let digit: BigInt = ((&amp;i * 27 - 12) * &amp;q + &amp;r * 5) / (&amp;t * 5);
        res.push(digit.to_i32().unwrap());
        let mut u: BigInt = &amp;i * 3;
        u = (&amp;u + 1) * 3 * (&amp;u + 2);
        r = &amp;u * 10 * (&amp;q * (&amp;i * 5 - 2) + r - &amp;t * digit);
        q *= 10 * &amp;i * (&amp;i * 2 - 1);
        i = i + 1;
        t *= u;
    }
    res
}
</code></pre>
<p>The whole Lambda project can be found on <a href="https://github.com/Ernyoke/aws-lambda-benchmarks" target="_blank">GitHub</a>.</p>
<p>The Lambda benchmarks and measurements were done with the following setup:</p>
<ul>
<li>The same Lambda was deployed with 4 types of configuration:<ul>
<li>
<code>x86-64</code> non-containerized;</li>
<li>
<code>arm64</code> non-containerized;</li>
<li>
<code>x86-64</code> using a Docker container;</li>
<li>
<code>arm64</code> using a Docker container;</li>
</ul>
</li>
<li>All the Lambdas had 128MB of memory allocated (the smallest setup possible);</li>
<li>All benchmarks were done in the <code>us-east-1</code> region.</li>
</ul>
<p>Each Lambda was invoked 5 times in a row with a custom event, which will require them to compute the first <code>10_000</code> digits of PI:</p>
<pre><code class="language-bash">aws lambda invoke \
    --function-name calculate-pi-rs-docker-arm64 \
    --invocation-type RequestResponse \
    --payload '{ "digits": "10_000" }' \
    --region us-east-1 \
    response.json
</code></pre>
<p>The results for the durations were taken from CloudWatch logs generated by each execution. After an execution, CloudWatch will contain an entry with the stats, for example:</p>
<pre><code>REPORT RequestId: fce00bdf-f1a0-4e6d-894e-34bfb3f861ae  Duration: 11079.90 ms   Billed Duration: 11098 ms   Memory Size: 128 MB Max Memory Used: 14 MB  Init Duration: 17.95 ms
</code></pre>
<p>I took the  <code>Duration</code> field from the log statements and I put it in a table. I repeated this 5 times, doing 5 executions in a row:</p>
<table>
<thead>
<tr>
<th></th>
<th>Duration - run 1</th>
<th>Duration - run 2</th>
<th>Duration - run 3</th>
<th>Duration - run 4</th>
<th>Duration - run 5</th>
</tr>
</thead>
<tbody>
<tr>
<td>x86-64</td>
<td>5454.09 ms</td>
<td>5334.88 ms</td>
<td>5364.77 ms</td>
<td>5386.38 ms</td>
<td>5314.65 ms</td>
</tr>
<tr>
<td>arm64</td>
<td>11064.26 ms</td>
<td>10900.01 ms</td>
<td>10909.01 ms</td>
<td>10929.45 ms</td>
<td>11204.16 ms</td>
</tr>
<tr>
<td>Docker x86-64</td>
<td>5501.72 ms</td>
<td>5734.71 ms</td>
<td>5337.14 ms</td>
<td>5580.38 ms</td>
<td>5206.17 ms</td>
</tr>
<tr>
<td>Docker arm64</td>
<td>11412.80 ms</td>
<td>12079.55 ms</td>
<td>12307.51 ms</td>
<td>12044.27 ms</td>
<td>11784.83 ms</td>
</tr>
</tbody>
</table>
<p><img alt="Rust implementation charts" src="img-running-serverless-lambads-with-rust/rust-pi-calculator.png"></p>
<p>The memory usage of these executions was not varying as much:
    - x86-64: 12-13 MB
    - arm64: 15-16 MB
    - Docker x86-64: 11-12 MB
    - Docker arm64: 11-12 MB</p>
<p>As a comparison, I measured the JavaScript implementation of the Unbounded Spigot Algorithm for computing the first 10k digits of PI. I got the following execution times:</p>
<table>
<thead>
<tr>
<th></th>
<th>Duration - run 1</th>
<th>Duration - run 2</th>
<th>Duration - run 3</th>
<th>Duration - run 4</th>
<th>Duration - run 5</th>
</tr>
</thead>
<tbody>
<tr>
<td>x86-64</td>
<td>24310.55 ms</td>
<td>24110.76 ms</td>
<td>23680.69 ms</td>
<td>24125.83 ms</td>
<td>24142.95 ms</td>
</tr>
<tr>
<td>arm64</td>
<td>27323.62 ms</td>
<td>27113.35 ms</td>
<td>27223.31 ms</td>
<td>27158.35 ms</td>
<td>26926.68 ms</td>
</tr>
<tr>
<td>Docker x86-64</td>
<td>24335.88 ms</td>
<td>24554.33 ms</td>
<td>23962.31 ms</td>
<td>24388.03 ms</td>
<td>24367.48 ms</td>
</tr>
<tr>
<td>Docker arm64</td>
<td>27752.90 ms</td>
<td>27042.74 ms</td>
<td>26617.62 ms</td>
<td>27020.76 ms</td>
<td>27064.53 ms</td>
</tr>
</tbody>
</table>
<p>A comparison between the Rust and the JavaScript implementations:</p>
<p><img alt="Rust - JS implementation comparison charts" src="img-running-serverless-lambads-with-rust/rust-vs-js-pi-calculator.png"></p>
<p>Moving on I measured the cold start of both Rust and JavaScript implementations. The cold start is the <code>Init</code> phase of a Lambda. We encounter a cold start if our Lambda was not invoked recently or AWS decides to fire up another instance of our Lambda function in parallel to which is running currently. To do the cold start measurements, I let all the Lambdas rest and executed them afterward. I noted down the cold start duration and I let all the Lambdas rest again to make sure to encounter a cold start at the next execution as well. I did this 3 times for each Lambda and I got the following results:</p>
<p>Rust:</p>
<table>
<thead>
<tr>
<th></th>
<th>Init - run 1</th>
<th>Init - run 2</th>
<th>Init - run 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>x86-64</td>
<td>21.14 ms</td>
<td>22.15 ms</td>
<td>22.38 ms</td>
</tr>
<tr>
<td>arm64</td>
<td>17.99 ms</td>
<td>17.25 ms</td>
<td>17.95 ms</td>
</tr>
<tr>
<td>Docker x86-64</td>
<td>17.05 ms</td>
<td>8.10 ms</td>
<td>119.34 ms</td>
</tr>
<tr>
<td>Docker arm64</td>
<td>15.14 ms</td>
<td>5.59 ms</td>
<td>114.04 ms</td>
</tr>
</tbody>
</table>
<p>JavaScript:</p>
<table>
<thead>
<tr>
<th></th>
<th>Init - run 1</th>
<th>Init - run 2</th>
<th>Init - run 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>x86-64</td>
<td>141.42 ms</td>
<td>147.52 ms</td>
<td>156.75 ms</td>
</tr>
<tr>
<td>arm64</td>
<td>139.01 ms</td>
<td>140.44 ms</td>
<td>140.62 ms</td>
</tr>
<tr>
<td>Docker x86-64</td>
<td>272.43 ms</td>
<td>252.28 ms</td>
<td>259.95 ms</td>
</tr>
<tr>
<td>Docker arm64</td>
<td>305.06 ms</td>
<td>277.63 ms</td>
<td>201.52 ms</td>
</tr>
</tbody>
</table>
<p><img alt="Rust/JS cold start measurement charts" src="img-running-serverless-lambads-with-rust/cold-start.png"></p>
<h3>Conclusions after these benchmarks:</h3>
<ul>
<li>Rust is fast and is faster than JavaScript if we have a computationally intensive task.</li>
<li>Running the same function in a Docker container and without containerizing yields the same running time. This is important since we can take advantage of containerized Lambda features without a performance penalty.</li>
<li>The most impressive results are the cold starts for Rust. Having a cold start around 20 ms would make us think twice about the necessity of allocating reserved concurrency. Moreover, the cold start for the dockerized Rust functions is all over the place. Having a sub 10 ms cold start and another one above 100 ms is strange, to say at least. </li>
</ul>
<h2>Final Conclusions<a class="anchor-link" href="#final-conclusions" id="final-conclusions">&lt;&lt;</a>
</h2>
<h3>Should I use Rust for my next production Lambda?</h3>
<p>Probably. I can affirm that the support and the tooling are there for Rust. AWS provides a supported implementation of the AWS SDK for Rust, which makes it usable in production code.</p>
<h3>Should I migrate my existing JavaScript/Python/Java/etc. Lambda to Rust?</h3>
<p>Probably not. While Rust is speedy if we are throwing computationally intensive tasks at it, in my experience this usually does not really happen for Lambda functions. Often, our Lambdas are IO bound, meaning we are waiting for some data from an API or we are reading/writing data to a database. In these cases, Rust may not make much of a difference performance-wise. Although, I could recommend Rust for data crunching and ETL jobs, or something like image processing. Nevertheless, what I suggest is to do some actual measurements before deciding to migrate from an existing solution.</p>
<h2>Links and References<a class="anchor-link" href="#links-and-references" id="links-and-references">&lt;&lt;</a>
</h2>
<ol>
<li>Lambda runtimes: <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html" target="_blank">AWS docs</a>
</li>
<li>Rust Runtime for AWS Lambda: <a href="https://aws.amazon.com/blogs/opensource/rust-runtime-for-aws-lambda/" target="_blank">AWS blogs</a>
</li>
<li>cargo-lambda: <a href="https://www.cargo-lambda.info/guide/what-is-cargo-lambda.html" target="_blank">cargo-lambda docs</a>
</li>
<li>cross: <a href="https://github.com/cross-rs/cross" target="_blank">GitHub</a>
</li>
<li>Container Image Support: <a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support" target="_blank">AWS blogs</a>
</li>
<li>Use multi-stage builds: <a href="https://docs.docker.com/develop/develop-images/multistage-build" target="_blank">Docker docs</a>
</li>
<li>Unbounded Spigot Algorithms for the Digits of PI: <a href="http://www.cs.ox.ac.uk/jeremy.gibbons/publications/spigot.pdf" target="_blank">http://www.cs.ox.ac.uk/jeremy.gibbons/publications/spigot.pdf</a>
</li>
</ol></article>
<div class="utterances">
</div>
<footer>
<span>Site generated with <a href="https://github.com/Ernyoke/ssg" target="_blank">ssg</a></span>
<span>|</span>
<span><a href="https://ervinszilagyi.dev/privacy.html">Privacy Policy</a></span>
<span>|</span>
<span><a href="https://ervinszilagyi.dev/ai.html">AI</a></span>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
<script src="https://ervinszilagyi.dev/scripts/terraform.js" type="text/javascript"></script>
<script src="https://ervinszilagyi.dev/scripts/dockerfile.min.js" type="text/javascript"></script>
<script>
        hljs.registerLanguage('terraform', window.hljsDefineTerraform);
        hljs.highlightAll();
    </script>
</body>
</html>
